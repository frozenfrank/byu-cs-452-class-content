{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/byu-cs-452/byu-cs-452-class-content/blob/main/embed/ConferenceScraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# You don't need to read all of this if you don't want to. Just \"run all\" if you want\" and wait for it to download."
      ],
      "metadata": {
        "id": "PoZRQbASYwYR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Required Libraries\n",
        "\n",
        "This cell installs the necessary Python libraries for web scraping, working with HTML content, and data manipulation.\n",
        "\n",
        "- `requests`: Used to fetch HTML content from web pages.\n",
        "- `beautifulsoup4`: Parses and extracts content from the HTML.\n",
        "- `PyPDF2`: If you need to work with PDF files.\n"
      ],
      "metadata": {
        "id": "225EVrFfYaRd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHwZi0Axx2ZD"
      },
      "outputs": [],
      "source": [
        "!pip install requests beautifulsoup4 PyPDF2\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Web Scraping Conference Talks\n",
        "\n",
        "This block contains helper functions for scraping LDS General Conference talks from the Church's website.\n",
        "\n",
        "- `get_soup()`: Sends a request and parses the HTML from a given URL.\n",
        "- `is_decade_page()`: Identifies if the URL is a decade selection page.\n",
        "- `scrape_conference_pages()`: Fetches URLs for each conference (April/October) from the main page.\n",
        "- `scrape_talk_urls()`: Retrieves URLs for individual talks from a specific conference.\n",
        "- `scrape_talk_data()`: Scrapes the detailed data for each talk, such as title, speaker, calling, year, and season.\n"
      ],
      "metadata": {
        "id": "3Z3FcXjAYgBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "import unicodedata\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def get_soup(url):\n",
        "    \"\"\"Create a tree structure (BeautifulSoup) out of a GET request's HTML.\"\"\"\n",
        "    try:\n",
        "        r = requests.get(url, allow_redirects=True)\n",
        "        r.raise_for_status()\n",
        "        print(f\"Successfully fetched {r.url}\")\n",
        "        return BeautifulSoup(r.content, \"html5lib\")\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error fetching {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def is_decade_page(url):\n",
        "    \"\"\"Check if a page is a decade selection page.\"\"\"\n",
        "    return bool(re.search(r\"/study/general-conference/\\d{4}\\d{4}\", url))\n",
        "\n",
        "def scrape_conference_pages(main_page_url):\n",
        "    \"\"\"Retrieve a list of URLs for each conference (year/month) from the main page.\"\"\"\n",
        "    soup = get_soup(main_page_url)\n",
        "    if soup is None:\n",
        "        print(f\"Failed to fetch content from {main_page_url}\")\n",
        "        return []\n",
        "\n",
        "    all_conference_links = []\n",
        "\n",
        "    # Find all the links to individual conferences or decades\n",
        "    links = [\n",
        "        \"https://www.churchofjesuschrist.org\" + a[\"href\"]\n",
        "        for a in soup.find_all(\"a\", href=True)\n",
        "        if re.search(r\"/study/general-conference/(\\d{4}/(04|10)|\\d{4}\\d{4})\", a[\"href\"])\n",
        "    ]\n",
        "\n",
        "    for link in links:\n",
        "        if is_decade_page(link):\n",
        "            # Handle decade page\n",
        "            decade_soup = get_soup(link)\n",
        "            if decade_soup:\n",
        "                year_links = [\n",
        "                    \"https://www.churchofjesuschrist.org\" + a[\"href\"]\n",
        "                    for a in decade_soup.find_all(\"a\", href=True)\n",
        "                    if re.search(r\"/study/general-conference/\\d{4}/(04|10)\", a[\"href\"])\n",
        "                ]\n",
        "                all_conference_links.extend(year_links)\n",
        "        else:\n",
        "            all_conference_links.append(link)\n",
        "            #print(f\"direct: {link}\")\n",
        "\n",
        "    print(f\"Total conference links found: {len(all_conference_links)}\")\n",
        "    print(\"Sample conference links:\", all_conference_links[:5])\n",
        "    print(all_conference_links)\n",
        "    #https://www.churchofjesuschrist.org/study/general-conference/2024/10?lang=eng\n",
        "    filtered_links = list(filter(lambda link: int(link.split(\"/\")[5]) >= 2018, all_conference_links))\n",
        "    #return all_conference_links\n",
        "    return filtered_links\n",
        "\n",
        "def scrape_talk_urls(conference_url):\n",
        "    \"\"\"Retrieve a list of URLs for each talk in a specific conference.\"\"\"\n",
        "    soup = get_soup(conference_url)\n",
        "    if soup is None:\n",
        "        return []\n",
        "\n",
        "    talk_links = [\n",
        "        \"https://www.churchofjesuschrist.org\" + a[\"href\"]\n",
        "        for a in soup.find_all(\"a\", href=True)\n",
        "        if re.search(r\"/study/general-conference/\\d{4}/(04|10)/.*\", a[\"href\"])\n",
        "    ]\n",
        "\n",
        "    # Remove duplicate links and session links\n",
        "    talk_links = list(set(talk_links))\n",
        "    talk_links = [link for link in talk_links if not link.endswith(\"session?lang=eng\")]\n",
        "\n",
        "    print(f\"Found {len(talk_links)} talk links in {conference_url}\")\n",
        "    if talk_links:\n",
        "        print(\"Sample talk links:\", talk_links[:3])\n",
        "    return talk_links\n",
        "\n",
        "def scrape_talk_data(url):\n",
        "    \"\"\"Scrapes a single talk for data such as: title, conference, calling, speaker, content.\"\"\"\n",
        "    try:\n",
        "        soup = get_soup(url)\n",
        "        if soup is None:\n",
        "            return {}\n",
        "\n",
        "        title_tag = soup.find(\"h1\")\n",
        "        title = title_tag.text.strip() if title_tag else \"No Title Found\"\n",
        "\n",
        "        conference_tag = soup.find(\"p\", {\"class\": \"subtitle\"})\n",
        "        conference = conference_tag.text.strip() if conference_tag else \"No Conference Found\"\n",
        "\n",
        "        author_tag = soup.find(\"p\", {\"class\": \"author-name\"})\n",
        "        speaker = author_tag.text.strip() if author_tag else \"No Speaker Found\"\n",
        "\n",
        "        calling_tag = soup.find(\"p\", {\"class\": \"author-role\"})\n",
        "        calling = calling_tag.text.strip() if calling_tag else \"No Calling Found\"\n",
        "\n",
        "        content_array = soup.find(\"div\", {\"class\": \"body-block\"})\n",
        "        content = \"\\n\\n\".join(paragraph.text.strip() for paragraph in content_array.find_all(\"p\")) if content_array else \"No Content Found\"\n",
        "\n",
        "        footnotes = \"\\n\".join(\n",
        "            f\"{idx}. {note.text.strip()}\" for idx, note in enumerate(soup.find_all(\"li\", {\"class\": \"study-note\"}), start=1)\n",
        "        ) if soup.find_all(\"li\", {\"class\": \"study-note\"}) else \"No Footnotes Found\"\n",
        "\n",
        "        year = re.search(r'/(\\d{4})/', url).group(1)\n",
        "        season = \"April\" if \"/04/\" in url else \"October\"\n",
        "\n",
        "        return {\n",
        "            \"title\": title,\n",
        "            \"speaker\": speaker,\n",
        "            \"calling\": calling,\n",
        "            \"year\": year,\n",
        "            \"season\": season,\n",
        "            \"url\": url,\n",
        "            \"talk\": content,\n",
        "            \"footnotes\": footnotes,\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to scrape {url}: {e}\")\n",
        "        return {}\n",
        "\n",
        "def scrape_talk_data_parallel(urls):\n",
        "    \"\"\"Scrapes all talks in parallel using ThreadPoolExecutor.\"\"\"\n",
        "    with ThreadPoolExecutor(max_workers=10) as executor:  # Adjust `max_workers` as needed\n",
        "        results = list(tqdm(executor.map(scrape_talk_data, urls), total=len(urls), desc=\"Scraping talks in parallel\"))\n",
        "    return [result for result in results if result]  # Filter out empty results\n",
        "\n",
        "def main_scrape_process():\n",
        "    main_url = \"https://www.churchofjesuschrist.org/study/general-conference?lang=eng\"\n",
        "    conference_urls = scrape_conference_pages(main_url)\n",
        "    print(\"Hello\")\n",
        "    print(conference_urls)\n",
        "    all_talk_urls = []\n",
        "    for conference_url in tqdm(conference_urls, desc=\"Scraping conferences\"):\n",
        "        all_talk_urls.extend(scrape_talk_urls(conference_url))\n",
        "\n",
        "    print(f\"Total talks found: {len(all_talk_urls)}\")\n",
        "\n",
        "    # Scrape talks in parallel\n",
        "    conference_talks = scrape_talk_data_parallel(all_talk_urls)\n",
        "\n",
        "    # Create DataFrame from the scraped data\n",
        "    conference_df = pd.DataFrame(conference_talks)\n",
        "\n",
        "    # Normalize Unicode and clean data\n",
        "    for col in conference_df.columns:\n",
        "        conference_df[col] = conference_df[col].apply(lambda x: unicodedata.normalize(\"NFD\", x) if isinstance(x, str) else x)\n",
        "        conference_df[col] = conference_df[col].apply(lambda x: x.replace(\"\\t\", \"\") if isinstance(x, str) else x)\n",
        "\n",
        "    # Save to CSV and JSON\n",
        "    conference_df.to_csv(\"conference_talks.csv\", index=False)\n",
        "    print(\"Scraping complete. Data saved to 'conference_talks.csv'.\")\n",
        "\n",
        "    conference_df.to_json(\"conference_talks.json\", orient=\"records\", indent=4)\n",
        "    print(\"Data also saved to 'conference_talks.json'.\")\n",
        "\n",
        "# Run the scraper\n",
        "start = time.time()\n",
        "main_scrape_process()\n",
        "end = time.time()\n",
        "print(f\"Total time taken: {end - start} seconds\")\n",
        "# main_url = \"https://www.churchofjesuschrist.org/study/general-conference?lang=eng\"\n",
        "# conference_urls = scrape_conference_pages(main_url)"
      ],
      "metadata": {
        "id": "WnesEHkCzBub",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('conference_talks.csv')\n"
      ],
      "metadata": {
        "id": "xUHjf5EE9Oz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clean Conference Talks Data\n",
        "\n",
        "This code block performs several cleaning operations on the scraped data:\n",
        "- Removes rows with \"Church Auditing Department\" in the calling.\n",
        "- Removes unnecessary columns, like \"conference\" and \"footnotes.\"\n",
        "- Standardizes titles for speakers and callings (e.g., \"Quorum of the 12\" and \"Seventy\").\n",
        "- Ensures uniformity across callings and speaker titles.\n",
        "- Saves the cleaned data to a CSV file for easier analysis and sharing.\n",
        "\n",
        "You can run this after scraping the data to ensure consistency and uniformity across the dataset.\n"
      ],
      "metadata": {
        "id": "KwxzUhLCYp3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def clean_conference_data(file_path):\n",
        "    # Load the CSV file\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Step 1: Remove rows with \"Church Auditing Department\" in the \"calling\" column\n",
        "    df = df[~df['calling'].str.contains(\"Church Auditing Department\", na=False)]\n",
        "\n",
        "    # Step 2: Remove the \"conference\" column\n",
        "    df = df.drop(columns=['conference'])\n",
        "\n",
        "    # Step 3: Modify callings to standardize common names\n",
        "    # Replace \"Quorum of the Twelve Apostles\", \"twelve\", or \"12\" with \"Quorum of the 12\"\n",
        "    df['calling'] = df['calling'].str.replace(r'Q_of_12|twelve|12|Council of the 12', 'Quorum of the 12', regex=True)\n",
        "\n",
        "    # Replace all variations of \"Seventy\" (including Quorum, Assistant, Former, Released) with \"Seventy\"\n",
        "    df['calling'] = df['calling'].str.replace(r'Q_of_70|70|Assistant to the Q_of_12|First Council of the Seventy|Presidency of the First Q_of_70|Emeritus member of the Seventy|Released Member of the Seventy', 'Seventy', regex=True)\n",
        "\n",
        "    # Standardize \"President of the Church\" across all variations\n",
        "    df['calling'] = df['calling'].str.replace(r'President of The Church of Jesus Christ of Latter-day Saints|President of the Church', 'President of the Church', regex=True)\n",
        "\n",
        "    # Step 4: Remove rows with \"Presented by\" in the speaker column\n",
        "    df = df[~df['speaker'].str.contains(\"Presented by\", na=False)]\n",
        "\n",
        "    # Step 5: Remove \"morning\", \"afternoon\", \"evening\" session titles (non-talk rows)\n",
        "    df = df[~df['title'].str.contains(r'morning|afternoon|evening', case=False, na=False)]\n",
        "\n",
        "    # Step 6: Standardize \"Former\" or variations in calling titles\n",
        "    df['calling'] = df['calling'].str.replace(r'Former member of the Seventy', 'Seventy', regex=True)\n",
        "\n",
        "    # Step 7: Remove \"Elder\", \"President\", \"Sister\", \"Brother\" from speaker names\n",
        "    df['speaker'] = df['speaker'].str.replace(r'Elder|President|Sister|Brother', '', regex=True).str.strip()\n",
        "\n",
        "    # Step 8: Remove the \"footnotes\" column as it was not needed\n",
        "    if 'footnotes' in df.columns:\n",
        "        df = df.drop(columns=['footnotes'])\n",
        "\n",
        "    # Save the cleaned data to CSV\n",
        "    df.to_csv(\"cleaned_conference_talks.csv\", index=False)\n",
        "    print(\"Data cleaned and saved to 'cleaned_conference_talks.csv'.\")\n",
        "\n",
        "# Example usage\n",
        "clean_conference_data('conference_talks.csv')\n"
      ],
      "metadata": {
        "id": "Lo1nq2mfX1S5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: I need to get embeddings for all the conference talks\n",
        "\n",
        "!pip install sentence-transformers\n",
        "\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "def generate_embeddings(csv_file):\n",
        "    \"\"\"Generates and adds sentence embeddings to a CSV file of conference talks.\"\"\"\n",
        "    try:\n",
        "        # Load the CSV file into a pandas DataFrame\n",
        "        df = pd.read_csv(csv_file)\n",
        "\n",
        "        # Initialize the sentence transformer model\n",
        "        model = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "        # Generate embeddings for the 'talk' column\n",
        "        df['embeddings'] = df['talk'].apply(lambda x: model.encode(x).tolist())\n",
        "\n",
        "        # Save the updated DataFrame to a new CSV file\n",
        "        df.to_csv(\"conference_talks_with_embeddings.csv\", index=False)\n",
        "        print(\"Embeddings generated and saved to 'conference_talks_with_embeddings.csv'\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Example usage\n",
        "generate_embeddings(\"conference_talks.csv\")\n"
      ],
      "metadata": {
        "id": "tZuMHRqGH-bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hs0Rvd7FOU4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: given a search term find the the top 3 most similar conference talks from the embeddings\n",
        "\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "def find_similar_talks(search_term, top_k=3):\n",
        "    \"\"\"\n",
        "    Finds the top_k most similar conference talks to a given search term based on embeddings.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(\"conference_talks_with_embeddings.csv\")\n",
        "\n",
        "        # Initialize the sentence transformer model (ensure it's the same as used for embedding generation)\n",
        "        model = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "        # Generate embedding for the search term\n",
        "        search_embedding = model.encode(search_term)\n",
        "\n",
        "        # Calculate cosine similarity between the search term embedding and talk embeddings\n",
        "        similarities = []\n",
        "        for index, row in df.iterrows():\n",
        "            talk_embedding = row['embeddings']  # Assuming 'embeddings' column contains the embeddings\n",
        "            if isinstance(talk_embedding, str): # Handle the case where embeddings are a string\n",
        "              talk_embedding = eval(talk_embedding) # Convert back to a list of floats\n",
        "            similarity = util.cos_sim(search_embedding, talk_embedding)\n",
        "            similarities.append((index, similarity.item())) # Store index and the similarity score\n",
        "\n",
        "        # Sort talks by similarity score in descending order\n",
        "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Get the top_k most similar talks\n",
        "        top_talks = []\n",
        "        for index, similarity in similarities[:top_k]:\n",
        "            top_talks.append({\n",
        "                \"title\": df.iloc[index]['title'],\n",
        "                \"speaker\": df.iloc[index]['speaker'],\n",
        "                \"year\": df.iloc[index]['year'],\n",
        "                \"similarity\": similarity,\n",
        "                \"text\": df.iloc[index]['talk']\n",
        "            })\n",
        "\n",
        "        return top_talks\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: 'conference_talks_with_embeddings.csv' not found.\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "# Example usage\n",
        "#search_term = \"building temples\"  # Replace with your search term\n",
        "search_term = \"serious depression\"  # Replace with your search term\n",
        "similar_talks = find_similar_talks(search_term)\n",
        "\n",
        "if similar_talks:\n",
        "    print(f\"Top 3 most similar talks to '{search_term}':\")\n",
        "    for talk in similar_talks:\n",
        "        print(f\"- Title: {talk['title']}, Speaker: {talk['speaker']}, Year: {talk['year']}, Similarity: {talk['similarity']:.4f}, {talk['text']}\")\n",
        "else:\n",
        "    print(\"No similar talks found.\")\n"
      ],
      "metadata": {
        "id": "DhsrXWsKOBXa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}